import os
import json
import gspread
import feedparser
from datetime import datetime
import time

# --- 1. Î— "ELITE" Î›Î™Î£Î¤Î‘ Î Î—Î“Î©Î (ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ¿Î¯, ÎÎ¿Î¼Î¹ÎºÎ¿Î¯, ÎšÏÎ¬Ï„Î¿Ï‚) ---
RSS_FEEDS = {
    # === Î˜Î•Î£ÎœÎ™ÎšÎŸÎ™ Î¦ÎŸÎ¡Î•Î™Î£ & ÎÎŸÎœÎŸÎ˜Î•Î£Î™Î‘ ===
    "ğŸ“œ E-Nomothesia (Î¦Î•Îš)": "https://www.e-nomothesia.gr/rss.xml",       # Î— ÎÎ¿1 Ï€Î·Î³Î® Î³Î¹Î± Î¦Î•Îš
    "ğŸ›ï¸ Î¤Î•Î• (ÎšÎµÎ½Ï„ÏÎ¹ÎºÏŒ)": "https://web.tee.gr/feed/",                       # Î¤ÎµÏ‡Î½Î¹ÎºÏŒ Î•Ï€Î¹Î¼ÎµÎ»Î·Ï„Î®ÏÎ¹Î¿
    "âš–ï¸ Î”Î¹ÎºÎ·Î³Î¿ÏÎ¹ÎºÏŒÏ‚ Î£ÏÎ»Î»Î¿Î³Î¿Ï‚ (Î”Î£Î‘)": "https://www.dsa.gr/rss.xml",         # Î”Î¹ÎºÎ·Î³Î¿ÏÎ¹ÎºÏŒÏ‚ Î£ÏÎ»Î»Î¿Î³Î¿Ï‚ Î‘Î¸Î·Î½ÏÎ½
    "ğŸ“ Dikaiologitika": "https://www.dikaiologitika.gr/feed",             # Î”Î¹Î¿Î¹ÎºÎ·Ï„Î¹ÎºÎ® ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ·
    "ğŸ’¼ Taxheaven": "https://www.taxheaven.gr/rss",                        # Î¦Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ® & Î•ÏÎ³Î±Ï„Î¹ÎºÎ® ÎÎ¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±

    # === ÎœÎ—Î§Î‘ÎÎ™ÎšÎŸÎ™ & ÎšÎ‘Î¤Î‘Î£ÎšÎ•Î¥Î•Î£ ===
    "ğŸ—ï¸ Ypodomes": "https://ypodomes.com/feed/",                           # Î”Î·Î¼ÏŒÏƒÎ¹Î± ÎˆÏÎ³Î± & Î¥Ï€Î¿Î´Î¿Î¼Î­Ï‚
    "ğŸŒ¿ B2Green": "https://news.b2green.gr/feed",                          # Î•Î½Î­ÏÎ³ÎµÎ¹Î±, Î•Î¾Î¿Î¹ÎºÎ¿Î½Î¿Î¼Ï, Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½
    "âš¡ EnergyPress": "https://energypress.gr/feed",                         # Î‘Î Î•, Î¡Î‘Î•, Î•Î½ÎµÏÎ³ÎµÎ¹Î±ÎºÎ® Î Î¿Î»Î¹Ï„Î¹ÎºÎ®
    "ğŸšœ PEDMEDE": "https://www.pedmede.gr/feed/",                          # Î•ÏÎ³Î¿Î»Î®Ï€Ï„ÎµÏ‚ Î”Î·Î¼Î¿ÏƒÎ¯Ï‰Î½ ÎˆÏÎ³Ï‰Î½
    "ğŸ‘· Michanikos Online": "https://www.michanikos-online.gr/feed/",      # Î¤ÎµÏ‡Î½Î¹ÎºÎ® Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ·

    # === ÎÎŸÎœÎ™ÎšÎŸÎ™ & Î‘ÎšÎ™ÎÎ—Î¤Î‘ ===
    "âš–ï¸ Lawspot": "https://www.lawspot.gr/nomika-nea/feed",                # ÎÎ¿Î¼Î¹ÎºÎ® Î•Ï€Î¹ÎºÎ±Î¹ÏÏŒÏ„Î·Ï„Î± & Î‘Î½Î±Î»ÏÏƒÎµÎ¹Ï‚
    "ğŸ  POMIDA (Î™Î´Î¹Î¿ÎºÏ„Î®Ï„ÎµÏ‚)": "https://www.pomida.gr/feed/",               # Î˜Î­Î¼Î±Ï„Î± Î‘ÎºÎ¹Î½Î®Ï„Ï‰Î½ & Î™Î´Î¹Î¿ÎºÏ„Î·ÏƒÎ¯Î±Ï‚
    "ğŸŒ GreenAgenda": "https://greenagenda.gr/feed/",                      # Î ÎµÏÎ¹Î²Î±Î»Î»Î¿Î½Ï„Î¹ÎºÏŒ Î”Î¯ÎºÎ±Î¹Î¿ & ÎšÏ…ÎºÎ»Î¹ÎºÎ® ÎŸÎ¹ÎºÎ¿Î½Î¿Î¼Î¯Î±
    "ğŸ“ Archetypes": "https://www.archetypes.gr/feed/",                    # Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ® & Design
    "ğŸ’° Capital (ÎŸÎ¹ÎºÎ¿Î½Î¿Î¼Î¯Î±)": "https://www.capital.gr/rss/oikonomia"       # ÎŸÎ¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÏŒ ÎšÎ»Î¯Î¼Î±
}

# --- 2. Î•ÎÎ¥Î ÎÎ— ÎšÎ‘Î¤Î—Î“ÎŸÎ¡Î™ÎŸÎ ÎŸÎ™Î—Î£Î— (Engineers vs Lawyers vs Notaries) ---
def guess_category(text):
    text = text.lower()
    
    # Î Î¡ÎŸÎ¤Î•Î¡Î‘Î™ÎŸÎ¤Î—Î¤Î‘ 1: ÎÎŸÎœÎŸÎ˜Î•Î£Î™Î‘ & Î¦Î•Îš (ÎšÎ¿Î¹Î½ÏŒ Î³Î¹Î± ÏŒÎ»Î¿Ï…Ï‚)
    if any(x in text for x in ['Ï†ÎµÎº', 'ÎµÎ³ÎºÏÎºÎ»Î¹Î¿Ï‚', 'Î½Î¿Î¼Î¿ÏƒÏ‡Î­Î´Î¹Î¿', 'Ï„ÏÎ¿Ï€Î¿Î»Î¿Î³Î¯Î±', 'ÎºÎ¿Î¹Î½Î® Ï…Ï€Î¿Ï…ÏÎ³Î¹ÎºÎ® Î±Ï€ÏŒÏ†Î±ÏƒÎ·', 'ÎºÏ…Î±', 'Ï€ÏÎ¿ÎµÎ´ÏÎ¹ÎºÏŒ Î´Î¹Î¬Ï„Î±Î³Î¼Î±', 'Î½ÏŒÎ¼Î¿Ï‚ Ï„Î¿Ï… ÎºÏÎ¬Ï„Î¿Ï…Ï‚']):
        return "ğŸ“œ ÎÎ¿Î¼Î¿Î¸ÎµÏƒÎ¯Î± & Î¦Î•Îš"

    # Î Î¡ÎŸÎ¤Î•Î¡Î‘Î™ÎŸÎ¤Î—Î¤Î‘ 2: ÎœÎ—Î§Î‘ÎÎ™ÎšÎŸÎ™
    elif any(x in text for x in ['Î±Ï…Î¸Î±Î¯ÏÎµÏ„Î±', '4495', 'Ï€Î¿Î»ÎµÎ¿Î´Î¿Î¼', 'Î´ÏŒÎ¼Î·ÏƒÎ·', 'ÎºÏ„Î¹ÏÎ¹Î¿Î´Î¿Î¼', 'Î±Î´ÎµÎ¹ÎµÏ‚', 'Î½.Î¿.Îº.', 'Î½Î¿Îº', 'Ï„Î¿Ï€Î¿Î³ÏÎ±Ï†Î¹Îº', 'Î·Î»ÎµÎºÏ„ÏÎ¿Î½Î¹ÎºÎ® Ï„Î±Ï…Ï„ÏŒÏ„Î·Ï„Î±', 'id ÎºÏ„Î¹ÏÎ¯Î¿Ï…']):
        return "ğŸ“ ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ¿Î¯: Î Î¿Î»ÎµÎ¿Î´Î¿Î¼Î¯Î±"
    elif any(x in text for x in ['ÎµÎ¾Î¿Î¹ÎºÎ¿Î½Î¿Î¼Ï', 'ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±', 'Ï†Ï‰Ï„Î¿Î²Î¿Î»Ï„Î±ÏŠÎºÎ¬', 'Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½', 'Î±Ï€Îµ', 'ÏÎ±Îµ', 'Ï…Î´ÏÎ¿Î³ÏŒÎ½Î¿', 'ÎºÎ»Î¹Î¼Î±Ï„Î¹Îº', 'ÎµÎ½ÎµÏÎ³ÎµÎ¹Î±Îº']):
        return "ğŸŒ± ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ¿Î¯: Î•Î½Î­ÏÎ³ÎµÎ¹Î±"
    elif any(x in text for x in ['Î´Î¹Î±Î³Ï‰Î½Î¹ÏƒÎ¼', 'Î´Î·Î¼ÏŒÏƒÎ¹Î± Î­ÏÎ³Î±', 'Î¼ÎµÎ»Î­Ï„ÎµÏ‚', 'ÏƒÏÎ¼Î²Î±ÏƒÎ·', 'Î±Î½Î¬Î¸ÎµÏƒÎ·', 'ÎµÏƒÏ€Î±', 'Ï…Ï€Î¿Î´Î¿Î¼Î­Ï‚', 'Ï€ÎµÎ´Î¼ÎµÎ´Îµ', 'Î¼ÎµÎ¹Î¿Î´Î¿Ï„']):
        return "âœ’ï¸ ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ¿Î¯: ÎˆÏÎ³Î±"
        
    # Î Î¡ÎŸÎ¤Î•Î¡Î‘Î™ÎŸÎ¤Î—Î¤Î‘ 3: ÎÎŸÎœÎ™ÎšÎŸÎ™ & Î£Î¥ÎœÎ’ÎŸÎ›Î‘Î™ÎŸÎ“Î¡Î‘Î¦ÎŸÎ™
    elif any(x in text for x in ['ÎºÏ„Î·Î¼Î±Ï„Î¿Î»ÏŒÎ³Î¹Î¿', 'Î´Î±ÏƒÎ¹Îº', 'ÏƒÏ…Î¼Î²Î¿Î»Î±Î¹Î¿Î³ÏÎ¬Ï†', 'Î¼ÎµÏ„Î±Î²Î¯Î²Î±ÏƒÎ·', 'Î³Î¿Î½Î¹ÎºÎ® Ï€Î±ÏÎ¿Ï‡Î®', 'ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¹', 'Î´Î¹Î±Î¸Î®ÎºÎ·', 'Î±Î½Ï„Î¹ÎºÎµÎ¹Î¼ÎµÎ½Î¹Îº', 'enfia', 'Ï…Ï€Î¿Î¸Î·ÎºÎ¿Ï†Ï…Î»Î±Îº']):
        return "ğŸ–‹ï¸ Î£Ï…Î¼Î²Î¿Î»Î±Î¹Î¿Î³ÏÎ±Ï†Î¹ÎºÎ¬ & Î‘ÎºÎ¯Î½Î·Ï„Î±"
    elif any(x in text for x in ['Î´Î¹ÎºÎ±ÏƒÏ„Î®ÏÎ¹', 'Î±ÏÎµÎ¿Ï€Î±Î³', 'ÏƒÏ…Î¼Î²Î¿ÏÎ»Î¹Î¿ Ï„Î·Ï‚ ÎµÏ€Î¹ÎºÏÎ±Ï„ÎµÎ¯Î±Ï‚', 'ÏƒÏ„Îµ', 'Ï€Î¿Î¹Î½Î¹Îº', 'Î±ÏƒÏ„Î¹Îº', 'Î´Î¯ÎºÎ·', 'Î±Î³Ï‰Î³Î®', 'Î´Î¹ÎºÎ·Î³ÏŒÏ', 'Î¿Î»Î¿Î¼Î­Î»ÎµÎ¹Î±', 'Î½Î¿Î¼Î¹ÎºÏŒ ÏƒÏ…Î¼Î²Î¿ÏÎ»Î¹Î¿']):
        return "âš–ï¸ ÎÎ¿Î¼Î¹ÎºÎ¬ Î˜Î­Î¼Î±Ï„Î± & Î”Î¹ÎºÎ±Î¹Î¿ÏƒÏÎ½Î·"
    
    # Î Î¡ÎŸÎ¤Î•Î¡Î‘Î™ÎŸÎ¤Î—Î¤Î‘ 4: ÎŸÎ™ÎšÎŸÎÎŸÎœÎ™ÎšÎ‘ & Î˜Î•Î£ÎœÎ™ÎšÎ‘
    elif any(x in text for x in ['Ï†Î¿ÏÎ¿Î»Î¿Î³', 'Î±Î±Î´Îµ', 'mydata', 'ÎµÏ†Î¿ÏÎ¯Î±', 'ÎµÎ¹ÏƒÏ†Î¿ÏÎ­Ï‚', 'Ï†Ï€Î±', 'Î¼Î¹ÏƒÎ¸Î¿Î´Î¿ÏƒÎ¯Î±', 'Ï„ÏÎ¬Ï€ÎµÎ¶ÎµÏ‚', 'Î´Î¬Î½ÎµÎ¹Î±', 'ÎµÏ†ÎºÎ±']):
        return "ğŸ’¼ Î¦Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ¬ & ÎŸÎ¹ÎºÎ¿Î½Î¿Î¼Î¯Î±"
    elif any(x in text for x in ['Ï„ÎµÎµ', 'ÎµÎºÎ»Î¿Î³Î­Ï‚', 'ÏƒÎµÎ¼Î¹Î½Î¬ÏÎ¹', 'ÏƒÏ…Î½Î­Î´ÏÎ¹Î¿', 'Ï€Î±ÏÎ¬Ï„Î±ÏƒÎ·', 'Î±Î½Î±ÎºÎ¿Î¯Î½Ï‰ÏƒÎ·', 'Î´ÎµÎ»Ï„Î¯Î¿ Ï„ÏÏ€Î¿Ï…']):
        return "ğŸ“¢ Î˜ÎµÏƒÎ¼Î¹ÎºÎ¬ & Î‘Î½Î±ÎºÎ¿Î¹Î½ÏÏƒÎµÎ¹Ï‚"
        
    else:
        return "ğŸŒ Î“ÎµÎ½Î¹ÎºÎ® Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ·"

# --- 3. Î— ÎœÎ—Î§Î‘ÎÎ— Î¤ÎŸÎ¥ Î¡ÎŸÎœÎ ÎŸÎ¤ ---
def run():
    print(f"ğŸ¤– [NomoTechi Bot] ÎÎµÎºÎ¹Î½Î¬ÎµÎ¹ ÏƒÎ¬ÏÏ‰ÏƒÎ· ÏƒÏ„Î¹Ï‚: {datetime.now()}")
    
    # Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ GitHub Secrets
    json_creds = os.environ.get("GCP_CREDENTIALS")
    if not json_creds:
        print("âŒ Î£Ï†Î¬Î»Î¼Î±: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎºÏ‰Î´Î¹ÎºÎ¿Î¯ (GCP_CREDENTIALS).")
        return

    # Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ Google Sheets
    try:
        creds_dict = json.loads(json_creds)
        gc = gspread.service_account_from_dict(creds_dict)
        sh = gc.open("laws_database")
        sheet = sh.sheet1
        print("âœ… Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ Î’Î¬ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÎµÏ€Î¹Ï„Ï…Ï‡Î®Ï‚.")
    except Exception as e:
        print(f"âŒ Critical Error: Î”ÎµÎ½ Î¼Ï€Î¿ÏÏ Î½Î± ÏƒÏ…Î½Î´ÎµÎ¸Ï ÏƒÏ„Î¿ Sheet. {e}")
        return

    # Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Ï…Ï€Î±ÏÏ‡ÏŒÎ½Ï„Ï‰Î½ Î³Î¹Î± Î±Ï€Î¿Ï†Ï…Î³Î® Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€Ï‰Î½
    try:
        existing_data = sheet.get_all_records()
        existing_links = [row['link'] for row in existing_data]
    except:
        existing_data = []
        existing_links = []
        print("âš ï¸ Î— Î²Î¬ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î¬Î´ÎµÎ¹Î± Î® Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Ï€ÏÏŒÎ²Î»Î·Î¼Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚.")
        
    new_items_count = 0

    # ÎšÎµÏ†Î±Î»Î¯Î´ÎµÏ‚ User-Agent (Î“Î¹Î± Î½Î± Ï†Î±Î¹Î½ÏŒÎ¼Î±ÏƒÏ„Îµ ÏƒÎ±Î½ Browser ÎºÎ±Î¹ ÏŒÏ‡Î¹ ÏƒÎ±Î½ Bot)
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

    # Î£Î¬ÏÏ‰ÏƒÎ· ÎºÎ¬Î¸Îµ Ï€Î·Î³Î®Ï‚
    for source_name, url in RSS_FEEDS.items():
        print(f"ğŸ“¡ Scanning: {source_name}...")
        try:
            # Î§ÏÎ®ÏƒÎ· Ï„Î¿Ï… 'agent' Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï… Î® headers Î±Î½ Ï‡ÏÎµÎ¹Î±ÏƒÏ„ÎµÎ¯
            feed = feedparser.parse(url, agent=headers['User-Agent'])
            
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Ï„Î¿ feed ÎµÎ¯Î½Î±Î¹ valid
            if hasattr(feed, 'bozo_exception') and feed.bozo_exception:
                 # ÎœÎµÏÎ¹ÎºÎ¬ feeds Î­Ï‡Î¿Ï…Î½ Î¼Î¹ÎºÏÎ¿Î»Î¬Î¸Î· Î±Î»Î»Î¬ Î´Î¿Ï…Î»ÎµÏÎ¿Ï…Î½, Ï„Î¿ ÎºÎ±Ï„Î±Î³ÏÎ¬Ï†Î¿Ï…Î¼Îµ ÎºÎ±Î¹ ÏƒÏ…Î½ÎµÏ‡Î¯Î¶Î¿Ï…Î¼Îµ
                 pass

            if not feed.entries:
                print(f"   âš ï¸ ÎšÎµÎ½ÏŒ feed Î® Î¼Ï€Î»Î¿ÎºÎ±ÏÎ¹ÏƒÎ¼Î­Î½Î¿: {source_name}")
                continue
                
            # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î± 5 Ï€Î¹Î¿ Ï€ÏÏŒÏƒÏ†Î±Ï„Î± Î¬ÏÎ¸ÏÎ± Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ Ï€Î·Î³Î®
            for entry in feed.entries[:5]: 
                if entry.link not in existing_links:
                    
                    # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¯Ï„Î»Î¿Ï… ÎºÎ±Î¹ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…
                    title = entry.title
                    summary = entry.summary if 'summary' in entry else ""
                    # Î‘Î½ Î· Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· Î­Ï‡ÎµÎ¹ HTML tags, Ï„Î± ÎºÏÎ±Ï„Î¬Î¼Îµ Î±Ï€Î»Î¬ (clean text ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»ÏÏ€Î»Î¿ÎºÎ¿ Ï‡Ï‰ÏÎ¯Ï‚ extra libs)
                    summary_clean = summary.replace("<p>", "").replace("</p>", "")[:250] + "..."
                    
                    category = guess_category(title + " " + summary_clean)
                    
                    new_row = [
                        len(existing_data) + new_items_count + 1, # ID
                        source_name,                              # Source
                        title,                                    # Title
                        summary_clean,                            # Content
                        entry.link,                               # Link
                        datetime.now().strftime("%Y-%m-%d"),      # Date (Fetch Date)
                        category                                  # Category
                    ]
                    
                    sheet.append_row(new_row)
                    new_items_count += 1
                    existing_links.append(entry.link)
                    print(f"   âœ… ÎÎ•ÎŸ: [{category}] {title[:40]}...")
                    
        except Exception as e:
            print(f"   âŒ Error scanning {source_name}: {e}")
            continue # Î£Ï…Î½ÎµÏ‡Î¯Î¶Î¿Ï…Î¼Îµ ÏƒÏ„Î·Î½ ÎµÏ€ÏŒÎ¼ÎµÎ½Î· Ï€Î·Î³Î®

    print(f"ğŸ ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ. Î ÏÎ¿ÏƒÏ„Î­Î¸Î·ÎºÎ±Î½ {new_items_count} Î½Î­Î± Î¸Î­Î¼Î±Ï„Î± ÏƒÏ„Î· Î²Î¬ÏƒÎ·.")

if __name__ == "__main__":
    run()
