import os
import json
import gspread
import feedparser
from datetime import datetime
import time
import re
import requests
from bs4 import BeautifulSoup
import random
import google.generativeai as genai # Î’Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ· Î³Î¹Î± Ï„Î¿ AI

# --- 1. CONFIG & API SETUP ---
# Î ÏÎ¿ÏƒÏ€Î±Î¸Î¿ÏÎ¼Îµ Î½Î± ÏƒÏ…Î½Î´ÎµÎ¸Î¿ÏÎ¼Îµ Î¼Îµ Ï„Î¿ AI
HAS_AI = False
try:
    api_key = os.environ.get("GOOGLE_API_KEY")
    if api_key:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-pro')
        HAS_AI = True
        print("âœ… Gemini AI Connected!")
    else:
        print("âš ï¸ No Google API Key found. Switching to Keyword Mode.")
except Exception as e:
    print(f"âš ï¸ AI Init Error: {e}")

RSS_FEEDS = {
    "ğŸ“œ E-Nomothesia": "https://www.e-nomothesia.gr/rss.xml",
    "âš–ï¸ Î”Î£Î‘": "https://www.dsa.gr/rss.xml",
    "âš–ï¸ Lawspot": "https://www.lawspot.gr/nomika-nea/feed",
    "ğŸ“ Dikaiologitika": "https://www.dikaiologitika.gr/feed", 
    "ğŸ’¼ Taxheaven": "https://www.taxheaven.gr/rss",
    "ğŸ›ï¸ Î¤Î•Î•": "https://web.tee.gr/feed/",
    "ğŸ—ï¸ Ypodomes": "https://ypodomes.com/feed/",
    "ğŸŒ¿ B2Green": "https://news.b2green.gr/feed",
    "âš¡ EnergyPress": "https://energypress.gr/feed",
    "ğŸšœ PEDMEDE": "https://www.pedmede.gr/feed/",
    "ğŸ‘· Michanikos": "https://www.michanikos-online.gr/feed/",
    "ğŸŒ GreenAgenda": "https://greenagenda.gr/feed/",
    "ğŸ  POMIDA": "https://www.pomida.gr/feed/",
    "ğŸ“ Archetypes": "https://www.archetypes.gr/feed/",
    "ğŸ’° Capital": "https://www.capital.gr/rss/oikonomia"
}

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

# --- 2. AI BRAIN (ÎŸ Î•Î“ÎšÎ•Î¦Î‘Î›ÎŸÎ£) ---
def ask_gemini_categories(title, summary):
    """Î¡Ï‰Ï„Î¬ÎµÎ¹ Ï„Î¿ AI ÏƒÎµ Ï€Î¿Î¹ÎµÏ‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ Î±Î½Î®ÎºÎµÎ¹ Ï„Î¿ Î¬ÏÎ¸ÏÎ¿"""
    if not HAS_AI: return None
    
    prompt = f"""
    Act as a professional editor for a Greek news portal for Engineers and Lawyers.
    Analyze this article title and summary:
    Title: {title}
    Summary: {summary}
    
    Assign it to one or more of these categories based on relevance:
    - ENGINEERS (if it's about construction, energy, urban planning, public works, real estate technicalities)
    - LEGAL (if it's about court decisions, lawyers, laws, justice, tax laws)
    - LEGISLATION (ONLY if it is a FEK, Law, Ministerial Decision, Circular)
    
    Return ONLY the categories separated by comma. Example: ENGINEERS, LEGISLATION
    """
    try:
        response = model.generate_content(prompt)
        return response.text.strip().upper()
    except:
        return None

# --- 3. CLASSIC LOGIC (BACKUP) ---
def guess_category_classic(title, summary, source_name):
    full_text = remove_accents(title + " " + summary)
    source_clean = remove_accents(source_name)
    categories = []

    # 1. Check for Legislation (Î¦Î•Îš)
    fek_keywords = ['Ï†ÎµÎº', 'ÎµÎ³ÎºÏ…ÎºÎ»Î¹Î¿Ï‚', 'ÎºÏ…Î±', 'Ï€ÏÎ¿ÎµÎ´ÏÎ¹ÎºÎ¿ Î´Î¹Î±Ï„Î±Î³Î¼Î±', 'Î½Î¿Î¼Î¿ÏƒÏ‡ÎµÎ´Î¹Î¿', 'Ï„ÏÎ¿Ï€Î¿Î»Î¿Î³Î¹Î±', 'Î±Ï€Î¿Ï†Î±ÏƒÎ·']
    if any(w in full_text for w in fek_keywords) or "e-nomothesia" in source_clean:
        categories.append("LEGISLATION")

    # 2. Check for Engineers/Real Estate
    eng_keywords = ['Î¼Î·Ï‡Î±Î½Î¹Îº', 'ÎµÏÎ³Î±', 'Î±ÎºÎ¹Î½Î·Ï„', 'Î´Î¿Î¼Î·ÏƒÎ·', 'Î±Ï…Î¸Î±Î¹ÏÎµÏ„Î±', 'ÎµÎ½ÎµÏÎ³ÎµÎ¹Î±', 'ÎµÎ¾Î¿Î¹ÎºÎ¿Î½Î¿Î¼Ï‰', 'ÎºÏ„Î·Î¼Î±Ï„Î¿Î»Î¿Î³Î¹Î¿', 'Ï€Î¿Î»ÎµÎ¿Î´Î¿Î¼', 'ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…', 'Ï…Ï€Î¿Î´Î¿Î¼ÎµÏ‚']
    if any(w in full_text for w in eng_keywords) or any(x in source_clean for x in ['b2green', 'ypodomes', 'tee', 'michanikos', 'pedmede', 'energy']):
        categories.append("ENGINEERS")

    # 3. Check for Legal
    law_keywords = ['Î´Î¹ÎºÎ±ÏƒÏ„Î·ÏÎ¹', 'Î´Î¹ÎºÎ·Î³Î¿Ï', 'ÏƒÏ…Î¼Î²Î¿Î»Î±Î¹Î¿Î³ÏÎ±Ï†', 'Î±ÏÎµÎ¿Ï€Î±Î³', 'ÏƒÏ„Îµ', 'Î½Î¿Î¼Î¹Îº', 'Î´Î¹ÎºÎ±Î¹Î¿ÏƒÏ…Î½Î·']
    if any(w in full_text for w in law_keywords) or any(x in source_clean for x in ['dsa', 'lawspot', 'taxheaven']):
        categories.append("LEGAL")

    # Default
    if not categories: categories.append("GENERAL")
    
    return ", ".join(categories)

# --- 4. HELPER FUNCTIONS ---
def fetch_article_image(url):
    try:
        headers = {'User-Agent': random.choice(USER_AGENTS)}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"): return og_image["content"]
    except: return ""
    return ""

def remove_accents(input_str):
    replacements = {'Î¬':'Î±','Î­':'Îµ','Î®':'Î·','Î¯':'Î¹','ÏŒ':'Î¿','Ï':'Ï…','Ï':'Ï‰'}
    for char, rep in replacements.items(): input_str = input_str.replace(char, rep)
    return input_str.lower()

def clean_summary(text):
    text = re.sub('<[^<]+?>', '', text)
    return text[:600] + "..."

# --- 5. MAIN LOOP ---
def run():
    print(f"ğŸ¤– [NomoTechi AI] Starting Scan...")
    
    json_creds = os.environ.get("GCP_CREDENTIALS")
    if not json_creds: return

    try:
        creds_dict = json.loads(json_creds)
        gc = gspread.service_account_from_dict(creds_dict)
        sh = gc.open("laws_database")
        sheet = sh.sheet1
        
        # Check Header
        if sheet.acell('H1').value != 'image_url': sheet.update_cell(1, 8, 'image_url')

    except Exception as e:
        print(f"Connection Error: {e}")
        return

    try:
        existing_data = sheet.get_all_records()
        existing_links = [row['link'] for row in existing_data]
    except:
        existing_data = []
        existing_links = []
        
    new_items_count = 0
    feed_headers = {'User-Agent': 'Mozilla/5.0'}

    for source_name, url in RSS_FEEDS.items():
        try:
            feed = feedparser.parse(url, agent=feed_headers['User-Agent'])
            if not feed.entries: continue
            
            for entry in feed.entries[:3]: 
                if entry.link not in existing_links:
                    title = entry.title
                    summary = clean_summary(entry.summary if 'summary' in entry else "")
                    
                    # --- AI DECISION ---
                    print(f"   ğŸ§  Analyzing: {title[:30]}...")
                    category = ask_gemini_categories(title, summary)
                    
                    # Fallback to classic if AI fails or key is missing
                    if not category:
                        category = guess_category_classic(title, summary, source_name)
                    
                    print(f"      ğŸ·ï¸ Tags: {category}")

                    real_image_url = fetch_article_image(entry.link)

                    new_row = [
                        len(existing_data) + new_items_count + 1,
                        source_name,
                        title,
                        summary,
                        entry.link,
                        datetime.now().strftime("%Y-%m-%d"),
                        category, # Multi-tag string (e.g. "ENGINEERS, LEGISLATION")
                        real_image_url
                    ]
                    
                    sheet.append_row(new_row)
                    new_items_count += 1
                    existing_links.append(entry.link)
        except Exception as e:
            print(f"Error on {source_name}: {e}")
            pass

    print(f"ğŸ Done. New articles: {new_items_count}")

if __name__ == "__main__":
    run()
