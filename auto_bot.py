import os
import json
import gspread
import feedparser
from datetime import datetime
import time
import re
import requests
from bs4 import BeautifulSoup

# --- 1. CONFIG ---
RSS_FEEDS = {
    "ğŸ“œ E-Nomothesia": "https://www.e-nomothesia.gr/rss.xml",
    "âš–ï¸ Î”Î£Î‘": "https://www.dsa.gr/rss.xml",
    "âš–ï¸ Lawspot": "https://www.lawspot.gr/nomika-nea/feed",
    "ğŸ“ Dikaiologitika": "https://www.dikaiologitika.gr/feed", 
    "ğŸ’¼ Taxheaven": "https://www.taxheaven.gr/rss",
    "ğŸ›ï¸ Î¤Î•Î•": "https://web.tee.gr/feed/",
    "ğŸ—ï¸ Ypodomes": "https://ypodomes.com/feed/",
    "ğŸŒ¿ B2Green": "https://news.b2green.gr/feed",
    "âš¡ EnergyPress": "https://energypress.gr/feed",
    "ğŸšœ PEDMEDE": "https://www.pedmede.gr/feed/",
    "ğŸ‘· Michanikos": "https://www.michanikos-online.gr/feed/",
    "ğŸŒ GreenAgenda": "https://greenagenda.gr/feed/",
    "ğŸ  POMIDA": "https://www.pomida.gr/feed/",
    "ğŸ“ Archetypes": "https://www.archetypes.gr/feed/",
    "ğŸ’° Capital": "https://www.capital.gr/rss/oikonomia"
}

def fetch_article_image(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"):
                return og_image["content"]
    except:
        return ""
    return ""

def remove_accents(input_str):
    replacements = {'Î¬':'Î±','Î­':'Îµ','Î®':'Î·','Î¯':'Î¹','ÏŒ':'Î¿','Ï':'Ï…','Ï':'Ï‰','Î†':'Î‘','Îˆ':'Î•','Î‰':'Î—','ÎŠ':'Î™','ÎŒ':'ÎŸ','Î':'Î¥','Î':'Î©','ÏŠ':'Î¹','Ï‹':'Ï…'}
    for char, rep in replacements.items():
        input_str = input_str.replace(char, rep)
    return input_str.lower()

def clean_summary(text):
    text = re.sub('<[^<]+?>', '', text)
    return text[:500] + "..."

def guess_category_smart(title, summary, source_name):
    full_text = remove_accents(title + " " + summary)
    source_clean = remove_accents(source_name)
    
    fek_keywords = ['Ï†ÎµÎº', 'ÎµÎ³ÎºÏ…ÎºÎ»Î¹Î¿Ï‚', 'ÎºÏ…Î±', 'Ï€ÏÎ¿ÎµÎ´ÏÎ¹ÎºÎ¿ Î´Î¹Î±Ï„Î±Î³Î¼Î±', 'Î½Î¿Î¼Î¿ÏƒÏ‡ÎµÎ´Î¹Î¿', 'Ï„ÏÎ¿Ï€Î¿Î»Î¿Î³Î¹Î±', 'Î±Ï€Î¿Ï†Î±ÏƒÎ· Ï…Ï€Î¿Ï…ÏÎ³Î¿Ï…']
    is_fek = any(w in full_text for w in fek_keywords) or "e-nomothesia" in source_clean

    if is_fek:
        eng_relevant_words = ['Î±Ï…Î¸Î±Î¹ÏÎµÏ„Î±', '4495', 'Ï€Î¿Î»ÎµÎ¿Î´Î¿Î¼', 'Î´Î¿Î¼Î·ÏƒÎ·', 'ÎºÏ„Î¹ÏÎ¹Î¿Î´Î¿Î¼', 'Î±Î´ÎµÎ¹ÎµÏ‚', 'Î¿Î¹ÎºÎ¿Î´Î¿Î¼', 'Î½Î¿Îº', 'Î´Î·Î¼Î¿ÏƒÎ¹Î± ÎµÏÎ³Î±', 'Î±Î½Î±Î¸ÎµÏƒÎ·', 'ÏƒÏ…Î¼Î²Î±ÏƒÎ·', 'Ï…Ï€Î¿Î´Î¿Î¼ÎµÏ‚', 'Î¼ÎµÏ„ÏÎ¿', 'Ï€ÎµÎ´Î¼ÎµÎ´Îµ', 'Î¼Î·Ï‡Î±Î½Î¹Îº', 'Ï„ÎµÎµ', 'ÎµÎ½ÎµÏÎ³ÎµÎ¹Î±Îº', 'ÎµÎ¾Î¿Î¹ÎºÎ¿Î½Î¿Î¼Ï‰', 'Î±Î½Ï„Î¹ÎºÎµÎ¹Î¼ÎµÎ½Î¹Îº', 'ÏƒÏ„Î±Ï„Î¹Îº', 'Î±Î½Ï„Î¹ÏƒÎµÎ¹ÏƒÎ¼Î¹Îº', 'ÏƒÎºÏ…ÏÎ¿Î´ÎµÎ¼']
        if any(w in full_text for w in eng_relevant_words):
            return "ğŸ“œ ÎÎ¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±: ÎœÎ·Ï‡Î±Î½Î¹ÎºÏÎ½ & ÎˆÏÎ³Ï‰Î½"
        return "ğŸ“œ ÎÎ¿Î¼Î¿Î¸ÎµÏƒÎ¯Î± & Î¦Î•Îš"

    scores = {"eng_poleodomia": 0, "eng_energy": 0, "eng_projects": 0, "law_realestate": 0, "law_justice": 0, "finance": 0, "news_general": 0}

    if "b2green" in source_clean or "greenagenda" in source_clean:
        scores["eng_energy"] += 3
    elif "ypodomes" in source_clean or "pedmede" in source_clean:
        scores["eng_projects"] += 3
    elif "pomida" in source_clean:
        scores["law_realestate"] += 3
    elif "lawspot" in source_clean or "dsa" in source_clean:
        scores["law_justice"] += 3
    elif "taxheaven" in source_clean or "capital" in source_clean:
        scores["finance"] += 3

    # Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎŸÎ™ Î’Î¡ÎŸÎ“Î§ÎŸÎ™ (Î£Ï‰ÏƒÏ„ÏŒ Indentation)
    poleodomia_words = ['Î±Ï…Î¸Î±Î¹ÏÎµÏ„Î±', '4495', 'Ï€Î¿Î»ÎµÎ¿Î´Î¿Î¼', 'Î´Î¿Î¼Î·ÏƒÎ·', 'ÎºÏ„Î¹ÏÎ¹Î¿Î´Î¿Î¼', 'Î±Î´ÎµÎ¹ÎµÏ‚', 'Î¿Î¹ÎºÎ¿Î´Î¿Î¼', 'Î½Î¿Îº', 'Ï„Î¿Ï€Î¿Î³ÏÎ±Ï†Î¹Îº', 'Ï„Î±Ï…Ï„Î¿Ï„Î·Ï„Î± ÎºÏ„Î¹ÏÎ¹Î¿Ï…', 'ÏƒÏ…Î½Ï„ÎµÎ»ÎµÏƒÏ„Î·Ï‚', 'Ï…Î´Î¿Î¼']
    for w in poleodomia_words:
        if w in full_text:
            scores["eng_poleodomia"] += 2
            
    energy_words = ['ÎµÎ¾Î¿Î¹ÎºÎ¿Î½Î¿Î¼Ï‰', 'Ï†Ï‰Ï„Î¿Î²Î¿Î»Ï„Î±Î¹Îº', 'ÎµÎ½ÎµÏÎ³ÎµÎ¹Î±', 'Î±Ï€Îµ', 'ÏÎ±Îµ', 'Ï…Î´ÏÎ¿Î³Î¿Î½Î¿', 'ÎºÎ»Î¹Î¼Î±Ï„Î¹Îº', 'Ï€ÎµÏÎ¹Î²Î±Î»Î»Î¿Î½', 'Î±Î½Î±ÎºÏ…ÎºÎ»Ï‰ÏƒÎ·', 'Î±Ï€Î¿Î²Î»Î·Ï„Î±']
    for w in energy_words:
        if w in full_text:
            scores["eng_energy"] += 2
            
    project_words = ['Î´Î¹Î±Î³Ï‰Î½Î¹ÏƒÎ¼', 'Î´Î·Î¼Î¿ÏƒÎ¹Î± ÎµÏÎ³Î±', 'Î±Î½Î±Î¸ÎµÏƒÎ·', 'ÏƒÏ…Î¼Î²Î±ÏƒÎ·', 'Ï…Ï€Î¿Î´Î¿Î¼ÎµÏ‚', 'Î¼ÎµÏ„ÏÎ¿', 'Î¿Î´Î¹ÎºÎ¿Ï‚', 'Ï€ÎµÎ´Î¼ÎµÎ´Îµ', 'Î¼ÎµÎ¹Î¿Î´Î¿Ï„', 'Î±Î½Î±Î´Î¿Ï‡Î¿Ï‚', 'ÎµÏÎ³Î¿Ï„Î±Î¾Î¹Î¿', 'ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÏ„Î¹Îº', 'Î³ÎµÏ†Ï…ÏÎ±', 'Î±Ï…Ï„Î¿ÎºÎ¹Î½Î·Ï„Î¿Î´ÏÎ¿Î¼Î¿Ï‚', 'ÏƒÎ¹Î´Î·ÏÎ¿Î´ÏÎ¿Î¼']
    for w in project_words:
        if w in full_text:
            scores["eng_projects"] += 2
            
    estate_words = ['ÏƒÏ…Î¼Î²Î¿Î»Î±Î¹Î¿Î³ÏÎ±Ï†', 'Î¼ÎµÏ„Î±Î²Î¹Î²Î±ÏƒÎ·', 'Î³Î¿Î½Î¹ÎºÎ· Ï€Î±ÏÎ¿Ï‡Î·', 'ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¹', 'Î´Î¹Î±Î¸Î·ÎºÎ·', 'Î±Î½Ï„Î¹ÎºÎµÎ¹Î¼ÎµÎ½Î¹Îº', 'enfia', 'Ï…Ï€Î¿Î¸Î·ÎºÎ¿Ï†Ï…Î»Î±Îº', 'ÎºÏ„Î·Î¼Î±Ï„Î¿Î»Î¿Î³Î¹Î¿', 'Îµ9', 'Î±ÎºÎ¹Î½Î·Ï„']
    for w in estate_words:
        if w in full_text:
            scores["law_realestate"] += 2

    disaster_words = ['Î·Ï†Î±Î¹ÏƒÏ„ÎµÎ¹Î¿', 'ÏƒÎµÎ¹ÏƒÎ¼Î¿Ï‚', 'Ï‡Î¹Î¿Î½Î¹Î±', 'ÎºÎ±ÎºÎ¿ÎºÎ±Î¹ÏÎ¹Î±', 'Ï€Ï…ÏÎºÎ±Î³Î¹Î±', 'Ï†Ï‰Ï„Î¹Î±', 'Ï€Î»Î·Î¼Î¼Ï…ÏÎ±', 'ÎºÎ±Î¹ÏÎ¿Ï‚']
    is_disaster = any(w in full_text for w in disaster_words)
    justice_words = ['Î´Î¹ÎºÎ±ÏƒÏ„Î·ÏÎ¹', 'Î±ÏÎµÎ¿Ï€Î±Î³', 'ÏƒÏ„Îµ', 'Ï€Î¿Î¹Î½Î¹Îº', 'Î±ÏƒÏ„Î¹Îº', 'Î´Î¹ÎºÎ·', 'Î±Î³Ï‰Î³Î·', 'Î´Î¹ÎºÎ·Î³Î¿Ï', 'Î¿Î»Î¿Î¼ÎµÎ»ÎµÎ¹Î±', 'Ï€Î±ÏÎ±Î²Î±ÏƒÎ·', 'ÎºÎ±Ï„Î·Î³Î¿ÏÎ¿Ï…Î¼ÎµÎ½']
    found_justice_words = sum(1 for w in justice_words if w in full_text)
    
    if is_disaster and found_justice_words < 2:
        scores["law_justice"] = -10 
    else:
        scores["law_justice"] += (found_justice_words * 2)

    fin_words = ['Ï†Î¿ÏÎ¿Î»Î¿Î³', 'Î±Î±Î´Îµ', 'mydata', 'ÎµÏ†Î¿ÏÎ¹Î±', 'Ï†Ï€Î±', 'Î¼Î¹ÏƒÎ¸Î¿Î´Î¿ÏƒÎ¹Î±', 'Ï„ÏÎ±Ï€ÎµÎ¶', 'Î´Î±Î½ÎµÎ¹', 'ÎµÏ†ÎºÎ±']
    for w in fin_words:
        if w in full_text:
            scores["finance"] += 2

    best_category = max(scores, key=scores.get)
    if scores[best_category] < 2:
        if any(w in full_text for w in ['ÎµÎºÎ»Î¿Î³ÎµÏ‚', 'Ï€Î±ÏÎ±Ï„Î±ÏƒÎ·', 'Î±Î½Î±ÎºÎ¿Î¹Î½Ï‰ÏƒÎ·']):
            return "ğŸ“¢ Î˜ÎµÏƒÎ¼Î¹ÎºÎ¬ & Î‘Î½Î±ÎºÎ¿Î¹Î½ÏÏƒÎµÎ¹Ï‚"
        return "ğŸŒ Î“ÎµÎ½Î¹ÎºÎ® Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ·"

    category_map = {
        "eng_poleodomia": "ğŸ“ ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ¿Î¯: Î Î¿Î»ÎµÎ¿Î´Î¿Î¼Î¯Î±",
        "eng_energy": "ğŸŒ± ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ¿Î¯: Î•Î½Î­ÏÎ³ÎµÎ¹Î± & Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½",
        "eng_projects": "âœ’ï¸ ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ¿Î¯: ÎˆÏÎ³Î±",
        "law_realestate": "ğŸ–‹ï¸ Î£Ï…Î¼Î²Î¿Î»Î±Î¹Î¿Î³ÏÎ±Ï†Î¹ÎºÎ¬ & Î‘ÎºÎ¯Î½Î·Ï„Î±",
        "law_justice": "âš–ï¸ ÎÎ¿Î¼Î¹ÎºÎ¬ Î˜Î­Î¼Î±Ï„Î±",
        "finance": "ğŸ’¼ Î¦Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ¬ & ÎŸÎ¹ÎºÎ¿Î½Î¿Î¼Î¯Î±",
        "news_general": "ğŸŒ Î“ÎµÎ½Î¹ÎºÎ® Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ·"
    }
    return category_map[best_category]

# --- 5. MAIN ---
def run():
    print(f"ğŸ¤– [NomoTechi AI] Scanning with Image Scraping...")
    json_creds = os.environ.get("GCP_CREDENTIALS")
    if not json_creds: return
    try:
        creds_dict = json.loads(json_creds)
        gc = gspread.service_account_from_dict(creds_dict)
        sh = gc.open("laws_database")
        sheet = sh.sheet1
    except Exception as e:
        print(f"Connection Error: {e}")
        return

    try:
        existing_data = sheet.get_all_records()
        existing_links = [row['link'] for row in existing_data]
    except:
        existing_data = []
        existing_links = []
        
    new_items_count = 0
    headers = {'User-Agent': 'Mozilla/5.0'}

    for source_name, url in RSS_FEEDS.items():
        try:
            feed = feedparser.parse(url, agent=headers['User-Agent'])
            if not feed.entries and feed.bozo: continue
            for entry in feed.entries[:3]: 
                if entry.link not in existing_links:
                    title = entry.title
                    summary = clean_summary(entry.summary if 'summary' in entry else "")
                    category = guess_category_smart(title, summary, source_name)
                    print(f"   ğŸ“¸ Getting image: {title[:20]}...")
                    real_image_url = fetch_article_image(entry.link)
                    new_row = [len(existing_data)+new_items_count+1, source_name, title, summary, entry.link, datetime.now().strftime("%Y-%m-%d"), category, real_image_url]
                    sheet.append_row(new_row)
                    new_items_count += 1
                    existing_links.append(entry.link)
        except:
            pass
    print(f"ğŸ Done. New: {new_items_count}")

if __name__ == "__main__":
    run()
